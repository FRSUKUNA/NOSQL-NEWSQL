#!/usr/bin/env python3
"""
Script pour g√©n√©rer des synth√®ses automatiques sur les innovations
(vector search, memory acceleration, etc.) bas√©es sur les changements.
"""

import json
import os
import shutil
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
import re

class InnovationSummaryGenerator:
    def __init__(self, output_dir: str = "output"):
        self.output_dir = Path(output_dir)
        self.backup_dir = Path("output_backup_innovations")
        
        # Cat√©gories d'innovations avec mots-cl√©s
        self.innovation_categories = {
            "vector_search": {
                "keywords": [
                    'vector', 'embedding', 'similarity', 'nearest neighbor', 'ann', 'approximate nearest',
                    'vector search', 'vector index', 'vector similarity', 'embedding search',
                    'faiss', 'hnsw', 'lsh', 'ivf', 'pq', 'product quantization',
                    'semantic search', 'vector database', 'vector storage', 'vector operations',
                    'dot product', 'cosine similarity', 'euclidean distance', 'manhattan distance',
                    'vector indexing', 'vector query', 'vector filter', 'vector aggregation',
                    'vector index type', 'vector distance', 'vector function', 'vector operator',
                    'vector storage engine', 'vector compression', 'vector encoding', 'vector decoding',
                    'vector normalization', 'vector quantization', 'vector clustering', 'vector partitioning'
                ],
                "description": "Recherche vectorielle et similarit√© s√©mantique",
                "examples": ["Vector indexing", "Embedding similarity search", "ANN algorithms"]
            },
            "memory_acceleration": {
                "keywords": [
                    'memory', 'cache', 'acceleration', 'ram', 'buffer', 'pool', 'allocation',
                    'memory optimization', 'memory management', 'memory pool', 'memory cache',
                    'in-memory', 'memory-mapped', 'mmap', 'shared memory', 'memory mapping',
                    'garbage collection', 'gc', 'heap', 'stack', 'memory leak', 'memory footprint',
                    'memory compression', 'memory deduplication', 'memory prefetching',
                    'tlb', 'translation lookaside buffer', 'memory hierarchy', 'memory bandwidth',
                    'memory allocator', 'memory arena', 'memory region', 'memory segment',
                    'memory controller', 'memory channel', 'memory bank', 'memory tier',
                    'memory tiering', 'memory hotness', 'memory cooling', 'memory eviction',
                    'memory reclamation', 'memory recycling', 'memory pooling', 'memory caching',
                    'fast memory', 'slow memory', 'persistent memory', 'non-volatile memory'
                ],
                "description": "Acc√©l√©ration m√©moire et optimisation cache",
                "examples": ["Memory pool optimization", "Cache acceleration", "Memory-mapped files"]
            },
            "ai_ml_integration": {
                "keywords": [
                    'ai', 'ml', 'machine learning', 'artificial intelligence', 'neural network',
                    'deep learning', 'model', 'inference', 'training', 'prediction',
                    'tensorflow', 'pytorch', 'onnx', 'model serving', 'ml pipeline',
                    'feature store', 'model registry', 'automl', 'mlops', 'model deployment',
                    'gpu', 'cuda', 'tensor', 'vectorization', 'batch processing', 'distributed training'
                ],
                "description": "Int√©gration IA/ML et machine learning",
                "examples": ["ML model integration", "AI-powered features", "Neural network inference"]
            },
            "distributed_computing": {
                "keywords": [
                    'distributed', 'cluster', 'shard', 'partition', 'replica', 'consensus',
                    'raft', 'paxos', 'gossip', 'leader election', 'load balancing',
                    'horizontal scaling', 'vertical scaling', 'elastic scaling', 'auto-scaling',
                    'microservices', 'service mesh', 'kubernetes', 'docker', 'container',
                    'parallel processing', 'concurrent', 'async', 'event-driven', 'stream processing'
                ],
                "description": "Calcul distribu√© et scalabilit√©",
                "examples": ["Distributed consensus", "Horizontal scaling", "Load balancing"]
            },
            "quantum_computing": {
                "keywords": [
                    'quantum', 'qubit', 'quantum computing', 'quantum algorithm', 'quantum circuit',
                    'quantum gate', 'quantum entanglement', 'quantum superposition',
                    'quantum annealing', 'quantum cryptography', 'quantum key distribution',
                    'quantum simulation', 'quantum optimization', 'quantum machine learning'
                ],
                "description": "Informatique quantique",
                "examples": ["Quantum algorithms", "Qubit operations", "Quantum cryptography"]
            },
            "blockchain_web3": {
                "keywords": [
                    'blockchain', 'web3', 'smart contract', 'decentralized', 'dapp', 'cryptocurrency',
                    'nft', 'token', 'wallet', 'consensus', 'proof of work', 'proof of stake',
                    'defi', 'dao', 'smart contract', 'ethereum', 'solidity', 'smart contract execution',
                    'distributed ledger', 'crypto', 'mining', 'staking', 'validation'
                ],
                "description": "Blockchain et technologies Web3",
                "examples": ["Smart contracts", "DeFi protocols", "NFT storage"]
            },
            "edge_computing": {
                "keywords": [
                    'edge', 'edge computing', 'iot', 'internet of things', 'edge device',
                    'fog computing', 'edge analytics', 'edge ai', 'edge inference',
                    'real-time processing', 'low latency', 'edge gateway', 'edge node',
                    'embedded systems', 'microcontroller', 'sensor', 'actuator', 'edge ml'
                ],
                "description": "Edge computing et IoT",
                "examples": ["Edge AI inference", "IoT data processing", "Real-time analytics"]
            },
            "security_privacy": {
                "keywords": [
                    'security', 'privacy', 'encryption', 'decryption', 'cryptography', 'zero-knowledge',
                    'homomorphic encryption', 'differential privacy', 'secure multi-party computation',
                    'privacy-preserving', 'anonymous', 'pseudonymous', 'confidential',
                    'access control', 'authentication', 'authorization', 'biometric', 'multi-factor',
                    'zero-trust', 'security by design', 'privacy by design'
                ],
                "description": "S√©curit√© avanc√©e et protection de la vie priv√©e",
                "examples": ["Zero-knowledge proofs", "Homomorphic encryption", "Privacy-preserving ML"]
            }
        }
    
    def detect_innovations(self, text: str) -> List[str]:
        """D√©tecte les cat√©gories d'innovations dans un texte"""
        detected = []
        text_lower = text.lower()
        
        for category, config in self.innovation_categories.items():
            # Compter le nombre de mots-cl√©s trouv√©s pour cette cat√©gorie
            keyword_matches = 0
            matched_keywords = []
            
            for keyword in config["keywords"]:
                if keyword in text_lower:
                    keyword_matches += 1
                    matched_keywords.append(keyword)
            
            # Ajouter la cat√©gorie seulement si on trouve au moins 2 mots-cl√©s
            # ou 1 mot-cl√© tr√®s sp√©cifique (plus de 3 caract√®res)
            if keyword_matches >= 2 or (keyword_matches >= 1 and any(len(kw) > 3 for kw in matched_keywords)):
                detected.append(category)
        
        return detected
    
    def extract_innovations_from_changes(self, changes: List[str]) -> List[Dict]:
        """Extrait les innovations des changements"""
        innovations = []
        for change in changes:
            detected_categories = self.detect_innovations(change)
            if detected_categories:
                innovations.append({
                    "description": change,
                    "categories": detected_categories,
                    "source": "changes"
                })
        return innovations
    
    def extract_innovations_from_ai_analysis(self, ai_analysis: Dict) -> List[Dict]:
        """Extrait les innovations de l'analyse IA"""
        innovations = []
        if 'details' in ai_analysis:
            for detail in ai_analysis['details']:
                detected_categories = self.detect_innovations(detail['description'])
                if detected_categories:
                    # Ne garder que les cat√©gories d√©tect√©es, pas les cat√©gories "unknown"
                    original_category = detail.get('category', '')
                    if original_category.lower() != 'unknown':
                        category = original_category
                    else:
                        # Utiliser la premi√®re cat√©gorie d√©tect√©e comme cat√©gorie principale
                        category = detected_categories[0]
                    
                    innovations.append({
                        "description": detail['description'],
                        "categories": detected_categories,
                        "category": category,
                        "source": "ai_analysis"
                    })
        return innovations
    
    def generate_innovation_summary(self, innovations: List[Dict]) -> Dict:
        """G√©n√®re une synth√®se des innovations"""
        # Compter par cat√©gorie
        category_counts = {}
        category_details = {}
        
        for innovation in innovations:
            for category in innovation['categories']:
                if category not in category_counts:
                    category_counts[category] = 0
                    category_details[category] = []
                
                category_counts[category] += 1
                
                # N'ajouter que les cat√©gories valides (pas "unknown")
                innovation_category = innovation.get('category', '')
                if innovation_category.lower() != 'unknown':
                    category_details[category].append({
                        "description": innovation['description'],
                        "source": innovation['source'],
                        "category": innovation_category
                    })
                else:
                    # Si la cat√©gorie est "unknown", utiliser la cat√©gorie d√©tect√©e
                    category_details[category].append({
                        "description": innovation['description'],
                        "source": innovation['source'],
                        "category": category
                    })
        
        # G√©n√©rer la synth√®se
        summary = {
            "total_innovations": len(innovations),
            "categories_detected": list(category_counts.keys()),
            "category_counts": category_counts,
            "category_details": category_details,
            "innovation_trends": self.analyze_trends(innovations),
            "top_innovations": self.get_top_innovations(innovations),
            "generation_date": datetime.now().isoformat()
        }
        
        return summary
    
    def analyze_trends(self, innovations: List[Dict]) -> Dict:
        """Analyse les tendances d'innovation"""
        trends = {}
        
        # Tendances par cat√©gorie
        category_frequency = {}
        for innovation in innovations:
            for category in innovation['categories']:
                if category not in category_frequency:
                    category_frequency[category] = 0
                category_frequency[category] += 1
        
        # Identifier les tendances √©mergentes
        total_innovations = len(innovations)
        emerging_trends = []
        established_trends = []
        
        for category, count in category_frequency.items():
            percentage = (count / total_innovations) * 100
            if percentage > 20:  # Plus de 20% = tendance √©tablie
                established_trends.append({
                    "category": category,
                    "count": count,
                    "percentage": round(percentage, 2)
                })
            elif percentage > 5:  # 5-20% = tendance √©mergente
                emerging_trends.append({
                    "category": category,
                    "count": count,
                    "percentage": round(percentage, 2)
                })
        
        return {
            "established_trends": established_trends,
            "emerging_trends": emerging_trends,
            "category_frequency": category_frequency
        }
    
    def get_top_innovations(self, innovations: List[Dict]) -> List[Dict]:
        """Extrait les innovations les plus importantes"""
        # Trier par nombre de cat√©gories (innovations multi-domaines)
        sorted_innovations = sorted(
            innovations, 
            key=lambda x: len(x['categories']), 
            reverse=True
        )
        
        # Prendre les 10 meilleures
        return sorted_innovations[:10]
    
    def process_version_data(self, version_data: Dict) -> Dict:
        """Traite les donn√©es d'une version pour ajouter les innovations"""
        modified_data = version_data.copy()
        
        # Extraire les innovations
        innovations = []
        
        # Depuis les changements
        if 'changes' in version_data:
            innovations.extend(self.extract_innovations_from_changes(version_data['changes']))
        
        # Depuis l'analyse IA
        if 'ai_analysis' in version_data:
            innovations.extend(self.extract_innovations_from_ai_analysis(version_data['ai_analysis']))
        
        # G√©n√©rer la synth√®se
        innovation_summary = self.generate_innovation_summary(innovations)
        
        # Ajouter la section innovation_summary
        modified_data['innovation_summary'] = innovation_summary
        
        return modified_data
    
    def backup_output_directory(self):
        """Cr√©e une sauvegarde du dossier output"""
        if self.backup_dir.exists():
            shutil.rmtree(self.backup_dir)
        shutil.copytree(self.output_dir, self.backup_dir)
        print(f"Sauvegarde cr√©√©e dans: {self.backup_dir}")
    
    def process_json_file(self, file_path: Path) -> bool:
        """Traite un fichier JSON individuel"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                print(f"Le fichier {file_path.name} ne contient pas une liste")
                return False
            
            modified = False
            for i, version_data in enumerate(data):
                if 'database' not in version_data:
                    continue
                
                # Traiter les donn√©es de la version
                modified_version = self.process_version_data(version_data)
                
                # V√©rifier si des modifications ont √©t√© apport√©es
                if modified_version != version_data:
                    data[i] = modified_version
                    modified = True
                    
                    innovation_count = modified_version['innovation_summary']['total_innovations']
                    categories_count = len(modified_version['innovation_summary']['categories_detected'])
                    
                    if innovation_count > 0:
                        version_name = version_data.get('patch_version', version_data.get('major_version', 'unknown'))
                        print(f"  {version_name}: {innovation_count} innovations dans {categories_count} cat√©gories")
                        
                        # Afficher les cat√©gories principales
                        top_categories = sorted(
                            modified_version['innovation_summary']['category_counts'].items(),
                            key=lambda x: x[1],
                            reverse=True
                        )[:3]
                        
                        for cat, count in top_categories:
                            print(f"    üöÄ {cat}: {count}")
            
            # Sauvegarder les modifications
            if modified:
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
                print(f"‚úÖ Fichier modifi√©: {file_path.name}")
                return True
            else:
                print(f"‚ÑπÔ∏è  Aucune innovation d√©tect√©e: {file_path.name}")
                return False
                
        except Exception as e:
            print(f"‚ùå Erreur lors du traitement du fichier {file_path}: {e}")
            return False
    
    def generate_global_summary(self):
        """G√©n√®re une synth√®se globale de toutes les innovations"""
        print("\nüåç G√âN√âRATION DE LA SYNTH√àSE GLOBALE...")
        
        all_innovations = []
        database_summaries = {}
        
        # Parcourir tous les fichiers
        for file_path in self.output_dir.glob("*.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                db_name = None
                for version_data in data:
                    if 'database' in version_data:
                        db_name = version_data['database']
                        break
                
                if db_name and 'innovation_summary' in version_data:
                    summary = version_data['innovation_summary']
                    database_summaries[db_name] = summary
                    all_innovations.extend(summary['category_details'].get('all', []))
                    
            except Exception as e:
                print(f"Erreur lors de la lecture de {file_path}: {e}")
        
        # G√©n√©rer la synth√®se globale
        global_summary = {
            "generation_date": datetime.now().isoformat(),
            "total_databases": len(database_summaries),
            "database_summaries": database_summaries,
            "global_trends": self.analyze_global_trends(database_summaries),
            "innovation_matrix": self.create_innovation_matrix(database_summaries)
        }
        
        # Sauvegarder la synth√®se globale
        with open("global_innovation_summary.json", "w", encoding="utf-8") as f:
            json.dump(global_summary, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ Synth√®se globale sauvegard√©e dans: global_innovation_summary.json")
        
        # Afficher un r√©sum√©
        self.print_global_summary(global_summary)
    
    def analyze_global_trends(self, database_summaries: Dict) -> Dict:
        """Analyse les tendances globales"""
        global_category_counts = {}
        
        for db_name, summary in database_summaries.items():
            for category, count in summary['category_counts'].items():
                if category not in global_category_counts:
                    global_category_counts[category] = {"total": 0, "databases": []}
                
                global_category_counts[category]["total"] += count
                global_category_counts[category]["databases"].append(db_name)
        
        return global_category_counts
    
    def create_innovation_matrix(self, database_summaries: Dict) -> Dict:
        """Cr√©e une matrice d'innovations par base de donn√©es"""
        matrix = {}
        
        for db_name, summary in database_summaries.items():
            matrix[db_name] = summary['category_counts']
        
        return matrix
    
    def print_global_summary(self, global_summary: Dict):
        """Affiche la synth√®se globale"""
        print("\n" + "=" * 80)
        print("üåç SYNTH√àSE GLOBALE DES INNOVATIONS")
        print("=" * 80)
        
        print(f"\nüìä Statistiques g√©n√©rales:")
        print(f"  Bases de donn√©es analys√©es: {global_summary['total_databases']}")
        
        print(f"\nüöÄ Tendances globales:")
        trends = global_summary['global_trends']
        sorted_trends = sorted(trends.items(), key=lambda x: x[1]['total'], reverse=True)
        
        for category, data in sorted_trends[:10]:
            print(f"  {category}: {data['total']} innovations dans {len(data['databases'])} bases de donn√©es")
    
    def test_innovation_detection(self):
        """Teste la d√©tection d'innovations"""
        print("\nüß™ TEST DE D√âTECTION D'INNOVATIONS")
        print("=" * 50)
        
        test_cases = [
            {
                "text": "Add vector index support for similarity search using HNSW algorithm",
                "expected_categories": ["vector_search"]
            },
            {
                "text": "Optimize memory pool allocation and reduce memory footprint",
                "expected_categories": ["memory_acceleration"]
            },
            {
                "text": "Implement garbage collection optimization and memory caching",
                "expected_categories": ["memory_acceleration"]
            },
            {
                "text": "Add embedding similarity search with cosine distance",
                "expected_categories": ["vector_search"]
            },
            {
                "text": "Fix minor bug in user interface",
                "expected_categories": []
            },
            {
                "text": "Memory-mapped file implementation for faster data access",
                "expected_categories": ["memory_acceleration"]
            }
        ]
        
        for i, test_case in enumerate(test_cases, 1):
            detected = self.detect_innovations(test_case["text"])
            expected = test_case["expected_categories"]
            
            print(f"\nTest {i}: {test_case['text'][:50]}...")
            print(f"  Attendu: {expected}")
            print(f"  D√©tect√©: {detected}")
            
            # V√©rifier si les cat√©gories attendues sont d√©tect√©es
            success = all(cat in detected for cat in expected)
            # V√©rifier si aucune cat√©gorie non d√©sir√©e n'est d√©tect√©e
            extra_categories = set(detected) - set(expected)
            
            if success and not extra_categories:
                print("  ‚úÖ Succ√®s")
            else:
                print("  ‚ùå √âchec")
                if extra_categories:
                    print(f"    Cat√©gories suppl√©mentaires: {extra_categories}")
    
    def process_all_files(self):
        """Traite tous les fichiers JSON du dossier output"""
        if not self.output_dir.exists():
            print(f"Le dossier {self.output_dir} n'existe pas.")
            return
        
        json_files = list(self.output_dir.glob("*.json"))
        if not json_files:
            print("Aucun fichier JSON trouv√© dans le dossier output.")
            return
        
        print(f"üöÄ Traitement de {len(json_files)} fichiers JSON pour d√©tection d'innovations...")
        print("=" * 80)
        
        # Cr√©er une sauvegarde
        self.backup_output_directory()
        
        total_files_modified = 0
        total_innovations = 0
        
        for file_path in json_files:
            print(f"\nüìÅ Traitement de: {file_path.name}")
            if self.process_json_file(file_path):
                total_files_modified += 1
                
                # Compter les innovations dans ce fichier
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    for version_data in data:
                        if 'innovation_summary' in version_data:
                            total_innovations += version_data['innovation_summary']['total_innovations']
                except:
                    pass
        
        print("\n" + "=" * 80)
        print("üöÄ R√âSUM√â DES INNOVATIONS")
        print("=" * 80)
        print(f"Fichiers trait√©s: {len(json_files)}")
        print(f"Fichiers avec innovations: {total_files_modified}")
        print(f"Total innovations d√©tect√©es: {total_innovations}")
        print(f"Sauvegarde disponible dans: {self.backup_dir}")
        
        if total_files_modified > 0:
            print("\n‚úÖ Tous les fichiers ont √©t√© mis √† jour avec les synth√®ses d'innovations!")
            # G√©n√©rer la synth√®se globale
            self.generate_global_summary()
        else:
            print("\n‚ÑπÔ∏è  Aucune innovation d√©tect√©e dans les fichiers.")

def main():
    """Fonction principale"""
    print("üöÄ G√©n√©ration de synth√®ses automatiques sur les innovations")
    print("Ce script va modifier directement les fichiers dans le dossier 'output'")
    print("üîç D√©tection automatique des innovations: vector search, memory acceleration, AI/ML, etc.")
    print("=" * 80)
    
    # Demander si l'utilisateur veut tester d'abord
    test_response = input("\nVoulez-vous tester la d√©tection d'innovations d'abord? (y/N): ").strip().lower()
    if test_response in ['y', 'yes', 'oui', 'o']:
        generator = InnovationSummaryGenerator()
        generator.test_innovation_detection()
        
        # Demander confirmation pour continuer
        continue_response = input("\nVoulez-vous continuer avec le traitement des fichiers? (y/N): ").strip().lower()
        if continue_response not in ['y', 'yes', 'oui', 'o']:
            print("Op√©ration annul√©e.")
            return
    
    # Demander confirmation pour le traitement principal
    response = input("\nVoulez-vous continuer avec le traitement des fichiers? (y/N): ").strip().lower()
    if response not in ['y', 'yes', 'oui', 'o']:
        print("Op√©ration annul√©e.")
        return
    
    generator = InnovationSummaryGenerator()
    generator.process_all_files()

if __name__ == "__main__":
    main()
